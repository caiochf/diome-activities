{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Objetive\n",
        "\n",
        "This notebook aims to show how to calculate the most famous metrics starting from a confusion matrix. Here are the metrics and their formulas:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\\begin{align} Recall = \\frac{TP}{TP + FN}\n",
        "    \\end{align}\n",
        "    \\\n",
        "\\begin{align} Specificity = \\frac{TN}{TN + TN}\n",
        "    \\end{align}\n",
        "    \\\n",
        "\\begin{align} Precision = \\frac{TP}{TP + FP}\n",
        "    \\end{align}\n",
        "    \\\n",
        "\\begin{align} Accuracy = \\frac{TP + TN}{TP+TN+FP+FN}\n",
        "    \\end{align}\n",
        "    \\\n",
        "\\begin{align} F-Score = 2 * \\frac{Precision * Recall}{Precision + Recall}\n",
        "    \\end{align}\n",
        "    \\\n",
        "\n",
        "Where:\n",
        "\n",
        "*   $TP = True \\ Positives$ (The afirmative prediction made by the model corresponds to the reality, or simply \"the model said it was something and it really was\");\n",
        "\n",
        "*   $TN = True \\ Negative$ (The model said that nothing was there or that input doens't belong to a specific class and it corresponds to the reality);\n",
        "\n",
        "*   $FN = False \\ Negative$ (the oposite of TN); and\n",
        "\n",
        "*   $FP = False \\ Positive$ (the oposite of TP).\n",
        "\n"
      ],
      "metadata": {
        "id": "T7hDjdwvFZKx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First of all, there is nothing amazing in this notebook in terms of coding... Our focus is to debate and share a few thoughts about the metrics and the context in which our confusion matrix was obtained."
      ],
      "metadata": {
        "id": "PBwL24e6Rem9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## Context\n",
        "\n",
        "A quick context for this study: in Brazil, there are several illegal activities happening at the Amazon Rainforest (or just \"Amazonia\"). Everything from illegal gold mining, drugs traffic and slavery happen there.\n",
        "\n",
        "All of this activity demands a huge logistic! Rivers, roads and even runways can be used to delivery products. We will focus on the aerial transport.\n",
        "\n",
        "For a plane to take off, it needs a runway. So a way to spot illegal activities is throught detectin illegal (or \"non registered\") runways made of dirt of grass at the Amazonia.\n",
        "\n",
        "In this [paper](https://ieeexplore.ieee.org/document/10648797) our friend was able to achieve amazing results at this task of detecting illegal runways. So, we used all of his affirmative predictions (or detections) and tried to filter even more, because despite his results being amazing, there was still plenty (numerically speaking) of FP.\n",
        "\n"
      ],
      "metadata": {
        "id": "mINUwR22Jqrw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, the following model was trained and tested:\n",
        "\n",
        "![Models Summary](model_summary.png)"],
      "metadata": {
        "id": "o_8keQ91TCkm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Details of the training will not be addressed here since it is not our main objective.\n",
        "\n",
        "The confusion matrix was obtained as follows:\n",
        "\n",
        "![Confusion Matrix](conf_matrix.png)"],
      "metadata": {
        "id": "eSBNEkRriHf_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Where:\n",
        "\n",
        "\"Pista\" = image with a runway(s) (positive detection).\n",
        "\n",
        "\"NÃ£o Pista\" = image without runway(s).\n",
        "\n",
        "So, from our matrix, we can infer:\n",
        "\n",
        "\\begin{align}\n",
        "    TP = 138 \\\\\n",
        "    TN = 1453 \\\\\n",
        "    FP = 392 \\\\\n",
        "    FN = 482\n",
        "  \\end{align}"
      ],
      "metadata": {
        "id": "kPOvRmXfjwQ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TP = 138\n",
        "TN = 1453\n",
        "FP = 392\n",
        "FN = 482"
      ],
      "metadata": {
        "id": "K8C9iWcsiCfL"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "recall = TP / (TP + FN)\n",
        "specificity = TN / (FP + TN)\n",
        "accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
        "precision = TP / (TP + FP)"
      ],
      "metadata": {
        "id": "sh755_Fpl7Nx"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f_score = 2 * (precision*recall) / (precision+recall)"
      ],
      "metadata": {
        "id": "OO9xKQ9Tmj_B"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Metrics: \\n Recall = {recall:.3f} \\n Specificity = {specificity:.3f} \\n Accuracy = {accuracy:.3f} \\n Precision = {precision:.3f} \\n F-Score = {f_score:.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TgE3rU8KmlAq",
        "outputId": "f6657061-3d76-46d1-c017-38ad28c6fa64"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metrics: \n",
            " Recall = 0.223 \n",
            " Specificity = 0.788 \n",
            " Accuracy = 0.645 \n",
            " Precision = 0.260 \n",
            " F-Score = 0.240\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## Discussion"
      ],
      "metadata": {
        "id": "LG9GtJ57oBFS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After performing our metrics calculation, here comes some thoughts:\n",
        "\n",
        "We can see that our model performs much better with images in which there are no targets. This behavior can be explained by the following points:\n",
        "\n",
        "\n",
        "*   Our classes are very unbalanced. Since most of the images of the Amazonia doesn't contain landstrips  (thank God...), our class containing those images are considerably smaller than the other one.\n",
        "\n",
        "*   We deployed a simple model made by hand and without transfer learning, so naturally its performance would be worst than the \"state-of-art\" models.\n",
        "\n"
      ],
      "metadata": {
        "id": "Ly8bRobhoETb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, lets take a look at the numbers and their meaning:\n",
        "\n",
        "\n",
        "*   **Recall** is the most importante metric here because our focus relies on detecting the maximun of landstripes as possible, even if that implies in the elevation of FP (this is the cancer detection argument). 22,3% is not a good result.\n",
        "\n",
        "*   **Specificity, precision and accuracy**: all of them tell us that our model classifies an image as \"Nao Pista\" almost all the time. This behavior can be explained by the points already exposed previously (unbalance and architecture).\n",
        "\n",
        "*   **F-Score** is obtained from precision and recall. Since both of them are bad, this one is also bad.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZQ9CsP-0pzoK"
      }
    }
  ]
}
